{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6c0253a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping page 1: HTTPConnectionPool(host='localhost', port=54405): Read timed out. (read timeout=120)\n",
      "Scraping page 2...\n",
      "Data saved to rightmove_data_page_2.csv\n",
      "Page 2 scraped successfully.\n",
      "Scraping page 3...\n",
      "Data saved to rightmove_data_page_3.csv\n",
      "Page 3 scraped successfully.\n",
      "Scraping page 4...\n",
      "Data saved to rightmove_data_page_4.csv\n",
      "Page 4 scraped successfully.\n",
      "Scraping page 5...\n",
      "Data saved to rightmove_data_page_5.csv\n",
      "Page 5 scraped successfully.\n",
      "Scraping page 6...\n",
      "Data saved to rightmove_data_page_6.csv\n",
      "Page 6 scraped successfully.\n",
      "Scraping page 7...\n",
      "Data saved to rightmove_data_page_7.csv\n",
      "Page 7 scraped successfully.\n",
      "Scraping page 8...\n",
      "Data saved to rightmove_data_page_8.csv\n",
      "Page 8 scraped successfully.\n",
      "Scraping page 9...\n",
      "Data saved to rightmove_data_page_9.csv\n",
      "Page 9 scraped successfully.\n",
      "Scraping page 10...\n",
      "Data saved to rightmove_data_page_10.csv\n",
      "Page 10 scraped successfully.\n",
      "Error scraping page 11: HTTPConnectionPool(host='localhost', port=54923): Read timed out. (read timeout=120)\n",
      "Scraping page 12...\n",
      "Data saved to rightmove_data_page_12.csv\n",
      "Page 12 scraped successfully.\n",
      "Scraping page 13...\n",
      "Data saved to rightmove_data_page_13.csv\n",
      "Page 13 scraped successfully.\n",
      "Error scraping page 14: HTTPConnectionPool(host='localhost', port=55088): Read timed out. (read timeout=120)\n",
      "Scraping page 15...\n",
      "Data saved to rightmove_data_page_15.csv\n",
      "Page 15 scraped successfully.\n",
      "Scraping page 16...\n",
      "Data saved to rightmove_data_page_16.csv\n",
      "Page 16 scraped successfully.\n",
      "Scraping page 17...\n",
      "Data saved to rightmove_data_page_17.csv\n",
      "Page 17 scraped successfully.\n",
      "Error scraping page 18: HTTPConnectionPool(host='localhost', port=55275): Read timed out. (read timeout=120)\n",
      "Scraping page 19...\n",
      "Data saved to rightmove_data_page_19.csv\n",
      "Page 19 scraped successfully.\n",
      "Scraping page 20...\n",
      "Data saved to rightmove_data_page_20.csv\n",
      "Page 20 scraped successfully.\n",
      "Error scraping page 21: HTTPConnectionPool(host='localhost', port=55427): Read timed out. (read timeout=120)\n",
      "Scraping page 22...\n",
      "Data saved to rightmove_data_page_22.csv\n",
      "Page 22 scraped successfully.\n",
      "Scraping page 23...\n",
      "Data saved to rightmove_data_page_23.csv\n",
      "Page 23 scraped successfully.\n",
      "Scraping page 24...\n",
      "Data saved to rightmove_data_page_24.csv\n",
      "Page 24 scraped successfully.\n",
      "Scraping page 25...\n",
      "Data saved to rightmove_data_page_25.csv\n",
      "Page 25 scraped successfully.\n",
      "Scraping page 26...\n",
      "Data saved to rightmove_data_page_26.csv\n",
      "Page 26 scraped successfully.\n",
      "Scraping page 27...\n",
      "Data saved to rightmove_data_page_27.csv\n",
      "Page 27 scraped successfully.\n",
      "Scraping page 28...\n",
      "Data saved to rightmove_data_page_28.csv\n",
      "Page 28 scraped successfully.\n",
      "Error scraping page 29: HTTPConnectionPool(host='localhost', port=55792): Read timed out. (read timeout=120)\n",
      "Scraping page 30...\n",
      "Data saved to rightmove_data_page_30.csv\n",
      "Page 30 scraped successfully.\n",
      "Scraping page 31...\n",
      "Data saved to rightmove_data_page_31.csv\n",
      "Page 31 scraped successfully.\n",
      "Scraping page 32...\n",
      "Data saved to rightmove_data_page_32.csv\n",
      "Page 32 scraped successfully.\n",
      "Scraping page 33...\n",
      "Data saved to rightmove_data_page_33.csv\n",
      "Page 33 scraped successfully.\n",
      "Scraping page 34...\n",
      "Data saved to rightmove_data_page_34.csv\n",
      "Page 34 scraped successfully.\n",
      "Scraping page 35...\n",
      "Error scraping page 35: HTTPConnectionPool(host='localhost', port=56058): Read timed out. (read timeout=120)\n",
      "Scraping page 36...\n",
      "Data saved to rightmove_data_page_36.csv\n",
      "Page 36 scraped successfully.\n",
      "Scraping page 37...\n",
      "Data saved to rightmove_data_page_37.csv\n",
      "Page 37 scraped successfully.\n",
      "Scraping page 38...\n",
      "Data saved to rightmove_data_page_38.csv\n",
      "Page 38 scraped successfully.\n",
      "Scraping page 39...\n",
      "Data saved to rightmove_data_page_39.csv\n",
      "Page 39 scraped successfully.\n",
      "Scraping page 40...\n",
      "Data saved to rightmove_data_page_40.csv\n",
      "Page 40 scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create database folder if it doesn't exist\n",
    "if not os.path.exists('database'):\n",
    "    os.makedirs('database')\n",
    "\n",
    "class RightmoveScraper:\n",
    "    def __init__(self):\n",
    "        # Setup Chrome driver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "    \n",
    "    def handle_cookies(self):\n",
    "        try:\n",
    "            cookie_button = self.driver.find_element(By.ID, \"onetrust-reject-all-handler\")\n",
    "            cookie_button.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def get_page_url(self, page):\n",
    "        base_url = \"https://www.rightmove.co.uk/house-prices/southwark-85215.html\"\n",
    "        return base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "\n",
    "    def scrape_page(self, page):\n",
    "        try:\n",
    "            # Load the page\n",
    "            self.driver.get(self.get_page_url(page))\n",
    "            \n",
    "            # Handle cookies on first page\n",
    "            if page == 1:\n",
    "                self.handle_cookies()\n",
    "            \n",
    "            # Wait for listings to load\n",
    "            listings = self.wait.until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH, '//div[@data-testid=\"propertyCard\"]')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Extract data from each listing\n",
    "            page_data = []\n",
    "            for listing in listings:\n",
    "                try:\n",
    "                    address = listing.find_element(By.CSS_SELECTOR, 'a.title.clickable').text.strip()\n",
    "                    price = listing.find_element(By.CSS_SELECTOR, 'td.price').text.strip()\n",
    "                    property_type = listing.find_element(By.CSS_SELECTOR, 'span.propertyType').text.strip()\n",
    "                    \n",
    "                    page_data.append({\n",
    "                        'address': address,\n",
    "                        'price': price,\n",
    "                        'property_type': property_type\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Save the data\n",
    "            if page_data:\n",
    "                df = pd.DataFrame(page_data)\n",
    "                filename = f'database/rightmove_data_page_{page}.csv'\n",
    "                df.to_csv(filename, index=False)\n",
    "                print(f\"Saved page {page} data - {len(page_data)} listings\")\n",
    "            \n",
    "            time.sleep(2)  # Basic delay between pages\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page}: {str(e)}\")\n",
    "\n",
    "    def combine_files(self):\n",
    "        try:\n",
    "            # Get all CSV files\n",
    "            all_files = [f for f in os.listdir('database') if f.startswith('rightmove_data_page_')]\n",
    "            \n",
    "            # Combine all files\n",
    "            all_data = []\n",
    "            for file in all_files:\n",
    "                df = pd.read_csv(f'database/{file}')\n",
    "                all_data.append(df)\n",
    "            \n",
    "            # Save combined data\n",
    "            if all_data:\n",
    "                combined_df = pd.concat(all_data, ignore_index=True)\n",
    "                combined_df.to_csv('database/combined_data.csv', index=False)\n",
    "                print(\"Created combined data file\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining files: {str(e)}\")\n",
    "\n",
    "    def run(self, start_page=1, end_page=40):\n",
    "        try:\n",
    "            for page in range(start_page, end_page + 1):\n",
    "                self.scrape_page(page)\n",
    "            self.combine_files()\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "def main():\n",
    "    scraper = RightmoveScraper()\n",
    "    scraper.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bac4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files have been combined into 'combined_database.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = 'database'\n",
    "\n",
    "# List to store each CSV file's data\n",
    "csv_data = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Read CSV file and append to the list\n",
    "        csv_data.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all CSV data into a single DataFrame\n",
    "combined_df = pd.concat(csv_data, ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_df.to_csv('combined_database.csv', index=False)\n",
    "\n",
    "print(\"All CSV files have been combined into 'combined_database.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373672d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
